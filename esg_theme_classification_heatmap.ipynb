{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG Theme Classification Heatmap Demo\n",
    "\n",
    "This notebook demonstrates how to classify ESG (Environmental, Social, Governance) themes in text and visualize the results as a heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries\n",
    "\n",
    "This notebook requires the following libraries (install via requirements.txt):\n",
    "- spacy (with en_core_web_sm)\n",
    "- transformers\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Document\n",
    "# Paste your CSR (Corporate Social Responsibility) report or similar text document below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Example CSR report text. \n",
    "This is a placeholder. Please replace this with your actual CSR report or text document.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Splitting with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the English language model (small version)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the input text with spaCy\n",
    "# This creates a Doc object with linguistic annotations\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Extract sentences using spaCy's sentence segmentation\n",
    "# spaCy identifies sentence boundaries based on punctuation and other linguistic features\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Print the list of sentences for verification\n",
    "print(f\"Found {len(sentences)} sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG Classification with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from transformers and torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pretrained ESG-BERT model and tokenizer from Hugging Face\n",
    "# This model is specifically trained to classify text into ESG categories\n",
    "model_name = \"nbroad/ESG-BERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify sentences into ESG categories\n",
    "def classify_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Classify a sentence into one of the ESG (Environmental, Social, Governance) categories.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The input sentence to classify\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (category_name, probability) where category_name is the predicted ESG category\n",
    "               and probability is the confidence score for that prediction\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the input sentence\n",
    "    # Convert the text into tokens that the model can understand\n",
    "    # return_tensors=\"pt\" returns PyTorch tensors\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Step 2: Feed the tokenized sentence into the model to obtain raw logits\n",
    "    # Set model to evaluation mode and disable gradient calculation for inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Step 3: Apply softmax function to convert logits into probability distribution\n",
    "    # Softmax normalizes the logits so they sum to 1, representing probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get the predicted class (highest probability)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    probability_value = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # Step 4: Map the predicted numerical label to ESG categories\n",
    "    # The mapping depends on how the model was trained\n",
    "    # For ESG-BERT, typically: 0=Environmental, 1=Social, 2=Governance\n",
    "    esg_categories = {\n",
    "        0: \"Environmental\",\n",
    "        1: \"Social\",\n",
    "        2: \"Governance\"\n",
    "    }\n",
    "    \n",
    "    category = esg_categories[predicted_class]\n",
    "    \n",
    "    return category, probability_value\n",
    "\n",
    "# Test the function on a sample sentence\n",
    "sample_sentence = \"The company reduced carbon emissions by 15% this year.\"\n",
    "category, confidence = classify_sentence(sample_sentence)\n",
    "print(f\"Sample: '{sample_sentence}'\")\n",
    "print(f\"Predicted ESG Category: {category}\")\n",
    "print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation of Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize counters for each ESG category\n",
    "# These will track how many sentences fall into each category\n",
    "environmental_count = 0\n",
    "social_count = 0\n",
    "governance_count = 0\n",
    "\n",
    "# Loop through each sentence in our list of sentences\n",
    "for sentence in sentences:\n",
    "    # Call the classify_sentence function to get the predicted ESG category\n",
    "    # This returns both the category name and the confidence score\n",
    "    category, confidence = classify_sentence(sentence)\n",
    "    \n",
    "    # Increment the appropriate counter based on the predicted category\n",
    "    if category == \"Environmental\":\n",
    "        environmental_count += 1\n",
    "    elif category == \"Social\":\n",
    "        social_count += 1\n",
    "    elif category == \"Governance\":\n",
    "        governance_count += 1\n",
    "\n",
    "# Create a dictionary with the counts for each category\n",
    "# This will be used to create our DataFrame\n",
    "esg_counts = {\n",
    "    \"Environmental\": [environmental_count],\n",
    "    \"Social\": [social_count],\n",
    "    \"Governance\": [governance_count]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame with the counts\n",
    "# The index is set to \"Report\" to indicate these counts are for the entire document\n",
    "esg_df = pd.DataFrame(esg_counts, index=[\"Report\"])\n",
    "\n",
    "# Print the resulting DataFrame showing the distribution of ESG categories\n",
    "print(\"ESG Category Distribution:\")\n",
    "print(esg_df)\n",
    "\n",
    "# Calculate and print the total number of sentences classified\n",
    "total_sentences = environmental_count + social_count + governance_count\n",
    "print(f\"\\nTotal sentences classified: {total_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap Visualization of ESG Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size for better visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "# - data: The DataFrame containing our ESG category counts\n",
    "# - annot=True: Display the numerical values in each cell\n",
    "# - cmap=\"YlGnBu\": Use the Yellow-Green-Blue color palette\n",
    "# - fmt=\"d\": Format annotations as integers (d = decimal integer)\n",
    "# - linewidths=.5: Add thin lines between cells for better separation\n",
    "# - cbar=True: Include a color bar legend\n",
    "ax = sns.heatmap(esg_df, \n",
    "                annot=True,        # Show the count values in each cell\n",
    "                cmap=\"YlGnBu\",     # Use the Yellow-Green-Blue color palette\n",
    "                fmt=\"d\",           # Format annotations as integers\n",
    "                linewidths=.5,     # Add thin lines between cells\n",
    "                cbar_kws={'label': 'Sentence Count'})  # Label the color bar\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title(\"ESG Theme Distribution in Report\", fontsize=14)\n",
    "\n",
    "# Customize the axis labels\n",
    "plt.xlabel(\"ESG Categories\", fontsize=12)\n",
    "plt.ylabel(\"Document\", fontsize=12)\n",
    "\n",
    "# Adjust layout to prevent clipping of labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Workflow: End-to-End ESG Classification and Visualization\n",
    "\n",
    "This cell integrates all the previous components into a complete workflow:\n",
    "1. Process the input text to extract individual sentences\n",
    "2. Classify each sentence into an ESG category (Environmental, Social, Governance)\n",
    "3. Aggregate the classification results into a structured DataFrame\n",
    "4. Visualize the distribution of ESG themes using a heatmap\n",
    "\n",
    "This workflow allows for quick analysis of ESG themes in corporate reports or other text documents,\n",
    "providing insights into the balance and emphasis of different sustainability aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Process the input text to split it into sentences\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(input_text)\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "print(f\"Processing {len(sentences)} sentences...\")\n",
    "\n",
    "# Step 2: Classify each sentence using the classify_sentence function\n",
    "# Initialize a list to store detailed results\n",
    "classification_results = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    category, confidence = classify_sentence(sentence)\n",
    "    classification_results.append({\n",
    "        'sentence': sentence,\n",
    "        'category': category,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "    \n",
    "# Step 3: Aggregate the classification results into a Pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Create a detailed DataFrame with all classification results\n",
    "results_df = pd.DataFrame(classification_results)\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = results_df['category'].value_counts().to_dict()\n",
    "\n",
    "# Ensure all categories are represented (even if count is 0)\n",
    "esg_counts = {\n",
    "    \"Environmental\": category_counts.get(\"Environmental\", 0),\n",
    "    \"Social\": category_counts.get(\"Social\", 0),\n",
    "    \"Governance\": category_counts.get(\"Governance\", 0)\n",
    "}\n",
    "\n",
    "# Create the aggregated DataFrame for visualization\n",
    "esg_df = pd.DataFrame([esg_counts], index=[\"Report\"])\n",
    "\n",
    "# Display the aggregated results\n",
    "print(\"\\nESG Category Distribution:\")\n",
    "print(esg_df)\n",
    "\n",
    "# Step 4: Generate and display the heatmap visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.heatmap(esg_df, \n",
    "                annot=True,\n",
    "                cmap=\"YlGnBu\",\n",
    "                fmt=\"d\",\n",
    "                linewidths=.5,\n",
    "                cbar_kws={'label': 'Sentence Count'})\n",
    "\n",
    "plt.title(\"ESG Theme Distribution in Report\", fontsize=14)\n",
    "plt.xlabel(\"ESG Categories\", fontsize=12)\n",
    "plt.ylabel(\"Document\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the final visualization\n",
    "plt.show()\n",
    "\n",
    "# Print a summary of the analysis\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "print(f\"Total sentences analyzed: {len(sentences)}\")\n",
    "print(f\"Environmental themes: {esg_counts['Environmental']} sentences ({esg_counts['Environmental']/len(sentences)*100:.1f}%)\")\n",
    "print(f\"Social themes: {esg_counts['Social']} sentences ({esg_counts['Social']/len(sentences)*100:.1f}%)\")\n",
    "print(f\"Governance themes: {esg_counts['Governance']} sentences ({esg_counts['Governance']/len(sentences)*100:.1f}%)\")\n",
    "print(f\"Dominant theme: {max(esg_counts, key=esg_counts.get)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
