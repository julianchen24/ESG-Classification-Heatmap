{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG Theme Classification Heatmap Demo\n",
    "\n",
    "This notebook demonstrates how to classify ESG (Environmental, Social, Governance) themes in text and visualize the results as a heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries\n",
    "\n",
    "This notebook requires the following libraries (install via requirements.txt):\n",
    "- spacy (with en_core_web_sm)\n",
    "- transformers\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Document\n",
    "# Paste your CSR (Corporate Social Responsibility) report or similar text document below\n",
    "\n",
    "input_text = \"\"\"Example CSR report text. \n",
    "This is a placeholder. Please replace this with your actual CSR report or text document.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Splitting with spaCy\n",
    "\n",
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the English language model (small version)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the input text with spaCy\n",
    "# This creates a Doc object with linguistic annotations\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Extract sentences using spaCy's sentence segmentation\n",
    "# spaCy identifies sentence boundaries based on punctuation and other linguistic features\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Print the list of sentences for verification\n",
    "print(f\"Found {len(sentences)} sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESG Classification with Transformers\n",
    "\n",
    "# Import necessary modules from transformers and torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pretrained ESG-BERT model and tokenizer from Hugging Face\n",
    "# This model is specifically trained to classify text into ESG categories\n",
    "model_name = \"nbroad/ESG-BERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify sentences into ESG categories\n",
    "def classify_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Classify a sentence into one of the ESG (Environmental, Social, Governance) categories.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The input sentence to classify\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (category_name, probability) where category_name is the predicted ESG category\n",
    "               and probability is the confidence score for that prediction\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the input sentence\n",
    "    # Convert the text into tokens that the model can understand\n",
    "    # return_tensors=\"pt\" returns PyTorch tensors\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Step 2: Feed the tokenized sentence into the model to obtain raw logits\n",
    "    # Set model to evaluation mode and disable gradient calculation for inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Step 3: Apply softmax function to convert logits into probability distribution\n",
    "    # Softmax normalizes the logits so they sum to 1, representing probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get the predicted class (highest probability)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    probability_value = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # Step 4: Map the predicted numerical label to ESG categories\n",
    "    # The mapping depends on how the model was trained\n",
    "    # For ESG-BERT, typically: 0=Environmental, 1=Social, 2=Governance\n",
    "    esg_categories = {\n",
    "        0: \"Environmental\",\n",
    "        1: \"Social\",\n",
    "        2: \"Governance\"\n",
    "    }\n",
    "    \n",
    "    category = esg_categories[predicted_class]\n",
    "    \n",
    "    return category, probability_value\n",
    "\n",
    "# Test the function on a sample sentence\n",
    "sample_sentence = \"The company reduced carbon emissions by 15% this year.\"\n",
    "category, confidence = classify_sentence(sample_sentence)\n",
    "print(f\"Sample: '{sample_sentence}'\")\n",
    "print(f\"Predicted ESG Category: {category}\")\n",
    "print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation of Classification Results\n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize counters for each ESG category\n",
    "# These will track how many sentences fall into each category\n",
    "environmental_count = 0\n",
    "social_count = 0\n",
    "governance_count = 0\n",
    "\n",
    "# Loop through each sentence in our list of sentences\n",
    "for sentence in sentences:\n",
    "    # Call the classify_sentence function to get the predicted ESG category\n",
    "    # This returns both the category name and the confidence score\n",
    "    category, confidence = classify_sentence(sentence)\n",
    "    \n",
    "    # Increment the appropriate counter based on the predicted category\n",
    "    if category == \"Environmental\":\n",
    "        environmental_count += 1\n",
    "    elif category == \"Social\":\n",
    "        social_count += 1\n",
    "    elif category == \"Governance\":\n",
    "        governance_count += 1\n",
    "\n",
    "# Create a dictionary with the counts for each category\n",
    "# This will be used to create our DataFrame\n",
    "esg_counts = {\n",
    "    \"Environmental\": [environmental_count],\n",
    "    \"Social\": [social_count],\n",
    "    \"Governance\": [governance_count]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame with the counts\n",
    "# The index is set to \"Report\" to indicate these counts are for the entire document\n",
    "esg_df = pd.DataFrame(esg_counts, index=[\"Report\"])\n",
    "\n",
    "# Print the resulting DataFrame showing the distribution of ESG categories\n",
    "print(\"ESG Category Distribution:\")\n",
    "print(esg_df)\n",
    "\n",
    "# Calculate and print the total number of sentences classified\n",
    "total_sentences = environmental_count + social_count + governance_count\n",
    "print(f\"\\nTotal sentences classified: {total_sentences}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
